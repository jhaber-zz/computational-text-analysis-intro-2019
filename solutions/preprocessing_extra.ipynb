{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Like other data types, text data never comes clean. Moreover, most of our downstream methods only accept data structured in a particular way. Because of this, before we do any computational text analysis techniques, we will always need to perform some level of preprocessing. Text data has its own unique kind of preprocessing. In this notebook, we will cover the core preprocessing methods in preparation for our next two weeks:\n",
    "\n",
    "- Reading in files\n",
    "- Character encoding\n",
    "- Tokenization\n",
    "- Sentence segmentation\n",
    "- Removing punctuation\n",
    "- **Stripping whitespace**\n",
    "- **Text normalization**\n",
    "- **Stop words**\n",
    "- **Stemming/Lemmatizing**\n",
    "- **POS tagging**\n",
    "- **DTM/TF-IDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenience functions for reading in today's data\n",
    "\n",
    "Here, we define a bunch of functions that simplify the process of reading in data that we'll use throughout today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = '../data'\n",
    "\n",
    "def read_pride():\n",
    "    fname = os.path.join(DATA_DIR, 'pride-and-prejudice.txt')\n",
    "    with open(fname) as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_trump():\n",
    "    fname = os.path.join(DATA_DIR, 'trump-tweets.csv')\n",
    "    df = pd.read_csv(fname)\n",
    "    return list(df['Tweet_Text'].values)\n",
    "\n",
    "def read_austen():\n",
    "    fnames = os.path.join(DATA_DIR, 'austen', '*.txt')\n",
    "    fnames = glob.glob(fnames)\n",
    "    austen = ''\n",
    "    for fname in fnames:\n",
    "        with open(fname) as f:\n",
    "            text = f.read()\n",
    "            austen += text\n",
    "    return austen\n",
    "\n",
    "def read_amazon(n=2):   \n",
    "    fnames = os.path.join(DATA_DIR, 'amazon', '*.csv')\n",
    "    fnames = glob.glob(fnames)\n",
    "    reviews = []\n",
    "    column_names = ['id', 'product_id', 'user_id', 'profile_name', 'helpfulness_num', 'helpfulness_denom',\n",
    "                   'score', 'time', 'summary', 'text']\n",
    "    for fname in fnames[:n]:\n",
    "        df = pd.read_csv(fname, names=column_names)\n",
    "        text = list(df['text'].iloc[1:])\n",
    "        reviews.extend(text)\n",
    "    return reviews\n",
    "\n",
    "def read_dante():\n",
    "    fname = os.path.join(DATA_DIR, 'dante.txt')\n",
    "    with open(fname) as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_example(n=1):\n",
    "    fname = os.path.join(DATA_DIR, 'example{}.txt'.format(n))\n",
    "    with open(fname) as f:\n",
    "        return f.read()\n",
    "    \n",
    "def read_music():\n",
    "    fname = os.path.join(DATA_DIR, 'music_reviews.csv')\n",
    "    return list(pd.read_csv(fname, sep='\\t')['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in files\n",
    "\n",
    "The first step is to read in the files containing the data. As we discussed last week, the most common file types for text data are: `.txt`, `.csv`, `.json`, `.html` and `.xml`.\n",
    "\n",
    "#### Reading in `.txt` files\n",
    "\n",
    "Python has built-in support for reading in `.txt` files.\n",
    "\n",
    "- What type of object is `raw`?\n",
    "- How many characters are in `raw`?\n",
    "- Get the first 1000 characters of `raw`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DATA_DIR = '../data'\n",
    "fname = 'pride-and-prejudice.txt'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "with open(fname, encoding='utf-8') as f:\n",
    "    raw = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in `.csv`\n",
    "\n",
    "Python has a built-in module called `csv` for reading in csv files.\n",
    "\n",
    "- What type is `tweets`?\n",
    "- How many entries are in `raw`?\n",
    "- Which entry is the header row?\n",
    "- How can we get the text of the first question?\n",
    "- How can we get a list of the texts of all questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "fname = 'trump-tweets.csv'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "tweets = []\n",
    "with open(fname) as f:\n",
    "    reader = csv.reader(f)\n",
    "    tweets = list(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in `.csv` with `pandas`\n",
    "\n",
    "`pandas` is a third-party library that makes working with tabular data much easier. This is the recommended way to read in a `.csv` file.\n",
    "\n",
    "- How many tweets are there?\n",
    "- What happened to the header row?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fname = 'trump-tweets.csv'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "tweets = pd.read_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Tweet_Text</th>\n",
       "      <th>Type</th>\n",
       "      <th>Media_Type</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Tweet_Id</th>\n",
       "      <th>Tweet_Url</th>\n",
       "      <th>twt_favourites_IS_THIS_LIKE_QUESTION_MARK</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>15:26:37</td>\n",
       "      <td>Today we express our deepest gratitude to all ...</td>\n",
       "      <td>text</td>\n",
       "      <td>photo</td>\n",
       "      <td>ThankAVet</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/797...</td>\n",
       "      <td>127213</td>\n",
       "      <td>41112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>13:33:35</td>\n",
       "      <td>Busy day planned in New York. Will soon be mak...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/797...</td>\n",
       "      <td>141527</td>\n",
       "      <td>28654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>11:14:20</td>\n",
       "      <td>Love the fact that the small groups of protest...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/797...</td>\n",
       "      <td>183729</td>\n",
       "      <td>50039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date      Time                                         Tweet_Text  \\\n",
       "0  16-11-11  15:26:37  Today we express our deepest gratitude to all ...   \n",
       "1  16-11-11  13:33:35  Busy day planned in New York. Will soon be mak...   \n",
       "2  16-11-11  11:14:20  Love the fact that the small groups of protest...   \n",
       "\n",
       "   Type Media_Type   Hashtags      Tweet_Id  \\\n",
       "0  text      photo  ThankAVet  7.970000e+17   \n",
       "1  text        NaN        NaN  7.970000e+17   \n",
       "2  text        NaN        NaN  7.970000e+17   \n",
       "\n",
       "                                           Tweet_Url  \\\n",
       "0  https://twitter.com/realDonaldTrump/status/797...   \n",
       "1  https://twitter.com/realDonaldTrump/status/797...   \n",
       "2  https://twitter.com/realDonaldTrump/status/797...   \n",
       "\n",
       "   twt_favourites_IS_THIS_LIKE_QUESTION_MARK  Retweets  Unnamed: 10  \\\n",
       "0                                     127213     41112          NaN   \n",
       "1                                     141527     28654          NaN   \n",
       "2                                     183729     50039          NaN   \n",
       "\n",
       "   Unnamed: 11  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today we express our deepest gratitude to all those who have served in our armed forces. #ThankAVet https://t.co/wPk7QWpK8Z',\n",
       " 'Busy day planned in New York. Will soon be making some very important decisions on the people who will be running our government!',\n",
       " 'Love the fact that the small groups of protesters last night have passion for our great country. We will all come together and be proud!',\n",
       " 'Just had a very open and successful presidential election. Now professional protesters, incited by the media, are protesting. Very unfair!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text = list(tweets['Tweet_Text'])\n",
    "tweet_text[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in `.json` files\n",
    "\n",
    "Python has built-in support for reading in `.json` files.\n",
    "\n",
    "- How many questions are there in the dataset?\n",
    "- What data type is each question?\n",
    "- How can we access the question text of the first question?\n",
    "- How can we get a list of the texts of all questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# fname = 'jeopardy.json'\n",
    "# fname = os.path.join(DATA_DIR, fname)\n",
    "# with open(fname) as f:\n",
    "#     data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in `.html` files\n",
    "\n",
    "The best way to read in `.html` files in Python is with the `BeautifulSoup` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "fname = 'time.html'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "with open(fname) as f:\n",
    "    html = f.read()\n",
    "    soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['html', '\\n', '\\n', '\\n', 'Time - Wikipedia']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = soup.findAll(text=True)\n",
    "#texts = soup.getText()\n",
    "texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in `.xml` files\n",
    "\n",
    "We read in `.xml` files using the `ElementTree` module of Python's standard library. We can think of `.xml` files as trees where each branch has a tag name. We can find all the branches with a certain name as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree as ET\n",
    "fname = 'books.xml'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "e = ET.parse(fname)\n",
    "root = e.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An in-depth look at creating applications \\n      with XML.',\n",
       " 'A former architect battles corporate zombies, \\n      an evil sorceress, and her own childhood to become queen \\n      of the world.',\n",
       " 'After the collapse of a nanotechnology \\n      society in England, the young survivors lay the \\n      foundation for a new society.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions = root.findall('*/description')\n",
    "text = [d.text for d in descriptions]\n",
    "text[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in multiple files\n",
    "\n",
    "Often, our text data is split across multiple files in a folder. We want to be able to read them all into a single variable.\n",
    "\n",
    "- What type is `austen`?\n",
    "- What type is `fnames` after it is first assigned a value?\n",
    "- What type is `fnames` after it is assigned a second value?\n",
    "- How "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "fnames = os.path.join(DATA_DIR, 'austen', '*.txt')\n",
    "fnames = glob.glob(fnames)\n",
    "austen = ''\n",
    "for fname in fnames:\n",
    "    with open(fname) as f:\n",
    "        text = f.read()\n",
    "        austen += text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Read in all the `.csv` files in the folder `amazon`. Extract out only the text column from each file and store them all in a list called `reviews`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character encoding\n",
    "\n",
    "Character encoding was more of a problem in Python 2 and early years in general. With Python 3 and most text files being encoded in `UTF-8`, we don't often need to think about it. If you're getting nonsense when reading in a file, try adding `encoding='utf-8'` to the `open` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'dante.txt'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "with open(fname) as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oglia.\\n\\n  Questi non ciberà terra né peltro,\\n  ma sapïenza, amore e virtute,\\n  e sua nazion sarà tra feltro e feltro.\\n\\n  Di quella umile Italia fia salute\\n  per cui morì la vergine Cammilla,\\n  Eurialo e Turno e Niso di ferute.\\n\\n  Questi la caccerà per ogne villa,\\n  fin che l’avrà rimessa ne lo ’nferno,\\n  là onde ’nvidia prima dipartilla.\\n\\n  Ond’ io per lo tuo me’ penso e discerno\\n  che tu mi segui, e io sarò tua guida,\\n  e trarrotti di qui per loco etterno;\\n\\n  ove udirai le disperate strida,\\n  vedrai li antichi spiriti dolenti,\\n  ch’a la seconda morte ciascun grida;\\n\\n  e vederai color che son contenti\\n  nel foco, perché speran di venire\\n  quando che sia a le beate genti.\\n\\n  A le quai poi se tu vorrai salire,\\n  anima fia a ciò più di me degna:\\n  con lei ti lascerò nel mio partire;\\n\\n  ché quello imperador che là sù regna,\\n  perch’ i’ fu’ ribellante a la sua legge,\\n  non vuol che ’n sua città per me si vegna.\\n\\n  In tutte parti impera e quivi regge;\\n  quivi è la sua città e l’alto seggio:\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[5000:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'akutagawa.txt'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "with open(fname) as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'二人は屍骸の中で、暫、無言のまま、つかみ合った。しか\\nし勝負は、はじめから、わかっている。下人はとうとう、老婆の腕をつかんで、無理に\\nそこへねじ倒した。丁度、鶏（とり）の脚のような、骨と皮ばかりの腕である。\\n\\u3000「何をしていた。さあ何をしていた。云え。云わぬと\\u3000これだぞよ。」\\n\\u3000下人は、老婆をつき放すと、いきなり、太刀の鞘を払って、白い鋼（はがね）の色を\\nその眼の前へつきつけた。けれども、老婆は黙っている。両手をわなわなふるわせて、\\n肩で息を切りながら、眼を、眼球がまぶたの外へ出そうになる程、見開いて、唖のよう\\nに執拗（しゅうね）く黙っている。これを見ると、下人は始めて明白にこの老婆の生死\\nが、全然、自分の意志に支配されていると云う事を意識した。そうして、この意識は、\\n今まではげしく燃えていた憎悪の心を何時（いつ）の間にか冷ましてしまった。後に残っ\\nたのは、唯、或仕事をして、それが円満に成就した時の、安らかな得意と満足とがある\\nばかりである。そこで、下人は、老婆を、見下げながら、少し声を柔げてこう云った。\\n\\u3000「己は検非違使（けびいし）の庁の役人などではない。今し方この門の下を通りかかっ\\nた旅の者だ。だからお前に縄をかけて、どうしようと云うような事はない。唯今時分、\\nこの門の上で、何をしていたのだか、それを己に話さえすればいいのだ。」\\n\\u3000すると、老婆は、見開いた眼を、一層大きくして、じっとその下人の顔を見守った。\\nまぶたの赤くなった、肉食鳥のような、鋭い眼で見たのである。それから、皺で、殆、\\n鼻と一つになった唇を何か物でも噛んでいるように動かした。細い喉で、尖った喉仏の\\n動いているのが見える。その時、その喉から、鴉（からす）の啼くような声が、喘ぎ喘\\nぎ、下人の耳へ伝わって来た。\\n\\u3000「この髪を抜いてな、この女の髪を抜いてな、鬘（かつら）にしようと思うたの\\nじゃ。」\\n\\u3000下人は、老婆の答が存外、平凡なのに失望した。そうして失望すると同時に、又前の\\n憎悪が、冷な侮蔑と一しょに、心の中へはいって来た。すると\\u3000その気色（けしき）が、\\n先方へも通じたのであろう。老婆は、片手に、まだ屍骸の頭から奪（と）った長い抜け\\n毛を持ったなり、蟇（ひき）のつぶやくような声で、口ごもりながら、こんな事を云っ\\nた。\\n\\u3000成程、死人の髪の毛を抜くと云う事は、悪い事かね知れぬ。しかし、こう云う死人の\\n多くは'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[5000:6000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Once we've read in the data, our next step is often to split it into words. This step is referred to as \"tokenization\". That's because each occurrence of a word is called a \"token\". Each distinct word used is called a word \"type\". So the word type \"the\" may correspond to multiple tokens of \"the\" in a text.\n",
    "\n",
    "#### Tokenizing by whitespace\n",
    "\n",
    "- What problems do you notice with tokenizing by whitespace?\n",
    "- What type is `text`?\n",
    "- What type is `tokens`?\n",
    "- What type is each element of `tokens`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "fname = 'example1.txt'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "with open(fname) as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'this',\n",
       " 'little',\n",
       " 'example,',\n",
       " \"we're\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'see',\n",
       " 'some',\n",
       " 'of']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing with regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In', 'this', 'little', 'example', 'we', 're', 'going', 'to', 'see', 'some']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "word_pattern = r'\\w+'\n",
    "tokens = re.findall(word_pattern, text)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing with `nltk`\n",
    "\n",
    "[Just a bunch of regular expressions under the hood](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/treebank.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In', 'this', 'little', 'example', ',', 'we', \"'re\", 'going', 'to', 'see']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk; nltk.download('punkt')\n",
    "tokens = word_tokenize(text)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge\n",
    "\n",
    "A while ago you read in a bunch of Jane Austen books into a variable called `austen`. Tokenize that using a method of your choice. Find all the unique words types (you might want the `set` function). Sort the resulting set object to create a vocabulary (you might want to use the `sorted` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence segmentation\n",
    "\n",
    "Sentence segmentation involves identifying the boundaries of sentences.\n",
    "\n",
    "#### Sentence segmentation by splitting on punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"In this little example, we're going to see some of the problems that regularly appear in tokenization\",\n",
       " \" Tokenization may seem simple, but it's harder than it first appears\",\n",
       " \" Why is it so hard? Punctuations, contractions (like don't, won't and would've) get in the way\",\n",
       " ' \\n']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could improve on this by using regular expressions. They'll allow us to split strings based on a number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"In this little example, we're going to see some of the problems that regularly appear in tokenization\",\n",
       " \" Tokenization may seem simple, but it's harder than it first appears\",\n",
       " ' Why is it so hard',\n",
       " \" Punctuations, contractions (like don't, won't and would've) get in the way\",\n",
       " ' \\n']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_boundary_pattern = r'[.?!]'\n",
    "re.split(sent_boundary_pattern, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "The file `example2.txt` has more punctuation problems. Read it in and see what the problems are. Try your best to modify the code from above to work for as many cases as you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'example2.txt'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "with open(fname) as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence segmentation by `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"In this little example, we're going to see some of the problems that regularly appear in tokenization.\",\n",
       " \"Tokenization may seem simple, but it's harder than it first appears.\",\n",
       " 'Why is it so hard?',\n",
       " \"Punctuations, contractions (like don't, won't and would've) get in the way.\",\n",
       " \"We can split text into sentences using punctuation, but unfortunately that's not always going to work.\",\n",
       " \"For example, if I wanted to tell you about Dr. Frankenstein, or Mrs. Doubtfire, we'd be in trouble.\",\n",
       " 'What if I wanted to write about U.C.',\n",
       " 'Berkeley?',\n",
       " 'When you think about it, URLs like www.google.com are troublesome too.',\n",
       " 'How would we settle on a price of $10.50?',\n",
       " 'The main point is that these punctuation characters serve a variety of purposes in writing.',\n",
       " 'Moreover, the functions they serve change depending on the domain (medical vs forum text) and language.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "fname = 'example2.txt'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "with open(fname) as f:\n",
    "    text = f.read()\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing punctuation\n",
    "\n",
    "Sometimes (although admittedly less frequently than tokenizing and sentence segmentation), you might want to keep only the alphanumeric characters (i.e. the letters and numbers) and ditch the punctuation. Here's how we can do that.\n",
    "\n",
    "- What type is `punctuation`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In this little example were going to see some of the problems that regularly appear in tokenization Tokenization may seem simple but its harder than it first appears Why is it so hard Punctuations contractions like dont wont and wouldve get in the way \\n\\nWe can split text into sentences using punctuation but unfortunately thats not always going to work For example if I wanted to tell you about Dr Frankenstein or Mrs Doubtfire wed be in trouble What if I wanted to write about UC Berkeley When you think about it URLs like wwwgooglecom are troublesome too How would we settle on a price of 1050 The main point is that these punctuation characters serve a variety of purposes in writing Moreover the functions they serve change depending on the domain medical vs forum text and language'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_punct = ''.join([ch for ch in text if ch not in punctuation])\n",
    "no_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strip whitespace\n",
    "\n",
    "This is an extremely common step. It's simple to perform and nicely pre-packaged in Python. It's particularly common for user-generated text (think survey forms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'example3.txt'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "with open(fname) as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "This is a text file that has some extra whitespace at the start and end. Whitespace is a catch-all term for spaces, tabs, newlines, and a bunch of other things that computers distinguish but to us all look like spaces, tabs and newlines.\n",
      "\n",
      "\n",
      "The Python method called \"strip\" only catches whitespace at the start and end of a string. But it won't catch it in       the middle,\t\tfor example,\n",
      "\n",
      "in this sentence.\t\tOnce again, regular expressions will\n",
      "\n",
      "help\t\tus    with this.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text file that has some extra whitespace at the start and end. Whitespace is a catch-all term for spaces, tabs, newlines, and a bunch of other things that computers distinguish but to us all look like spaces, tabs and newlines.\n",
      "\n",
      "\n",
      "The Python method called \"strip\" only catches whitespace at the start and end of a string. But it won't catch it in       the middle,\t\tfor example,\n",
      "\n",
      "in this sentence.\t\tOnce again, regular expressions will\n",
      "\n",
      "help\t\tus    with this.\n"
     ]
    }
   ],
   "source": [
    "stripped_text = text.strip()\n",
    "print(stripped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This is a text file that has some extra whitespace at the start and end. Whitespace is a catch-all term for spaces, tabs, newlines, and a bunch of other things that computers distinguish but to us all look like spaces, tabs and newlines. The Python method called \"strip\" only catches whitespace at the start and end of a string. But it won\\'t catch it in the middle, for example, in this sentence. Once again, regular expressions will help us with this. '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whitespace_pattern = r'\\s+'\n",
    "clean_text = re.sub(whitespace_pattern, ' ', text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revision\n",
    "\n",
    "I've read in the text of Jane Austen's _Pride and Prejudice_ into a variable called `pride`. Your tasks are to:\n",
    "- Figure out what type of Python object `pride` is.\n",
    "- Tokenize the text and store it in a variable called `tokenized_pride`.\n",
    "- Figure out what type `tokenized_pride` is.\n",
    "- Remove all punctuation from `pride`.\n",
    "- Remove all punctuation from `tokenized_pride`.\n",
    "- Break `pride` up into sentences and store the result as `sents_pride`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pride = read_pride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text normalization\n",
    "\n",
    "Text normalization means making our text fit some standard patterns. Lots of steps come under this wide umbrella, but the most common are:\n",
    "\n",
    "- case folding\n",
    "- removing URLs, digits, hashtags\n",
    "- OOV (removing infequent words)\n",
    "\n",
    "#### Case folding\n",
    "\n",
    "Case folding means dealing with upper and lower cases characters. This is usually done by making all characters lower cased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_example(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#['One', 'Two'].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "The `lower` method we used above is a string method, that is, it works on strings. But what if you want to lowercase every word in a list (say you've already tokenized the text). Take the list of tokens below and make each one lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing URLs, digits and hashtags\n",
    "\n",
    "We rarely care about the exact URL used in a tweet, or the exact number. We could remove them completely (think about how we'd do that), but it's often informative to know that there is a URL or a digit in the text. So we want to replace individual URLs asnd digits with a symbol that preserves the fact that a URL was there. It's standard to just use the strings \"URL\" and \"DIGIT\".\n",
    "\n",
    "How do we do this? Once again, regular expressions save the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today we express our deepest gratitude to all those who have served in our armed forces. #ThankAVet https://t.co/wPk7QWpK8Z',\n",
       " 'Busy day planned in New York. Will soon be making some very important decisions on the people who will be running our government!',\n",
       " 'Love the fact that the small groups of protesters last night have passion for our great country. We will all come together and be proud!',\n",
       " 'Just had a very open and successful presidential election. Now professional protesters, incited by the media, are protesting. Very unfair!',\n",
       " 'A fantastic day in D.C. Met with President Obama for first time. Really good meeting, great chemistry. Melania liked Mrs. O a lot!']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = read_trump()\n",
    "tweets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today we express our deepest gratitude to all those who have served in our armed forces. #ThankAVet https://t.co/wPk7QWpK8Z'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_pattern = r'https?:\\/\\/.*[\\r\\n]*'\n",
    "single_tweet = tweets[0]\n",
    "single_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today we express our deepest gratitude to all those who have served in our armed forces. #ThankAVet  URL '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL_SIGN = ' URL '\n",
    "re.sub(url_pattern, URL_SIGN, single_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge\n",
    "\n",
    "Above we replaced the URL in a single tweet. Now replace all the URLs in all tweets in `tweet_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge\n",
    "\n",
    "Use the regular expression for hashtags below to replace all hashtags in all tweets in `tweet_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_pattern = r'(?:^|\\s)[＃#]{1}(\\w+)'\n",
    "HASHTAG_SIGN = ' HASHTAG '\n",
    "digit_pattern = '\\d+'\n",
    "DIGIT_SIGN = ' DIGIT '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OOV words\n",
    "\n",
    "Sometimes it's best for us to remove infrequent words (sometimes not!). When we do remove infrequent words, it's often for a downstream method (like classification) that is sensitive to rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today',\n",
       " 'we',\n",
       " 'express',\n",
       " 'our',\n",
       " 'deepest',\n",
       " 'gratitude',\n",
       " 'to',\n",
       " 'all',\n",
       " 'those',\n",
       " 'who',\n",
       " 'have',\n",
       " 'served',\n",
       " 'in',\n",
       " 'our',\n",
       " 'armed',\n",
       " 'forces',\n",
       " 'HASHTAG',\n",
       " 'URL',\n",
       " 'HASHTAG',\n",
       " 'HASHTAG']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets = ' '.join(tweets)\n",
    "clean = re.sub(url_pattern, URL_SIGN, all_tweets)\n",
    "clean = re.sub(hashtag_pattern, HASHTAG_SIGN, clean)\n",
    "clean = re.sub(digit_pattern, DIGIT_SIGN, clean)\n",
    "tokens = word_tokenize(clean)\n",
    "tokens = [token for token in tokens if token not in punctuation]\n",
    "tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count the frequency of each word type with the built-in `Counter` in Python. This basically just takes the set of word types (we calculated this above as `vocabulary`) and makes a special Python dictionary with each value being the number of times it appears in the list. We can ask that dictionary for the most common words, or for the frequency of individual word types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('URL', 932),\n",
       " ('HASHTAG', 717),\n",
       " ('DIGIT', 258),\n",
       " ('the', 87),\n",
       " ('in', 76),\n",
       " ('to', 72),\n",
       " ('of', 61),\n",
       " ('you', 57),\n",
       " ('I', 56),\n",
       " ('is', 54)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "freq = Counter(tokens)\n",
    "freq.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq['unleashed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "OOV = 'OOV'\n",
    "new_tokens = []\n",
    "for token in tokens:\n",
    "    if freq[token] == 1:\n",
    "        new_tokens.append(OOV)\n",
    "    else:\n",
    "        new_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OOV',\n",
       " 'we',\n",
       " 'OOV',\n",
       " 'our',\n",
       " 'OOV',\n",
       " 'OOV',\n",
       " 'to',\n",
       " 'all',\n",
       " 'those',\n",
       " 'who',\n",
       " 'have',\n",
       " 'OOV',\n",
       " 'in',\n",
       " 'our',\n",
       " 'OOV',\n",
       " 'OOV',\n",
       " 'HASHTAG',\n",
       " 'URL',\n",
       " 'HASHTAG',\n",
       " 'HASHTAG']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "I've read in some Amazon reviews from earlier into a list called `reviews`. Each element of the list is a string, representing the text of a single review. Try to:\n",
    "- Tokenize each review\n",
    "- Separate each review into sentences\n",
    "- Strip all whitespace\n",
    "- Make all characters lower case\n",
    "- Replace any URLs and digits\n",
    "\n",
    "Then find the most common 50 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"OKAY HOW SHOULD I SAY THIS, IT DOES NOT TASTE LIKE CREAM CHEESE...<br />IT HAS A WEIRD ALMOST CHEMICAL TASTE TO IT, I EVEN STORED IT IN THE FRIG TO KEEP IT FRESH, IT DID NOT HELP...<br />THING IS I HAD USED IT BEFORE YEARS AGO AND I HAD THOUGHT IT WAS GREAT THEN...<br />MAYBE I GOT A BAD BATCH OR MAYBE THEY CHANGED HOW IT BEING MADE NOW..<br />I KNOW I WON'T BE BUYING IT AGAIN..............\",\n",
       " 'I buy pistachios a LOT. When I say a lot, I mean about once a month or so. And as far as I\\'m concerned Keenan is the best. I get very few \"bad ones\" in the entire container, whether it\\'s this one or the plastic bags I also buy. The saltiness is perfect, the size is perfect, and Keenan really knows how to sell in bulk. I just don\\'t bother looking at other brand names, though I am sure they are fine. But another thing to note is that Keenan\\'s tend to be cheapest per pound. I did all the math for all the brands here on Amazon and I just get the most for my money with these or the large 32-ounce bags.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = read_amazon()\n",
    "reviews[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stop words\n",
    "\n",
    "You might have noticed that the most common words above aren't terribly exciting. They're words like \"am\", \"i\", \"the\" and \"a\": stop words. These are rarely useful to us in computational text analysis, so it's very common to remove them completely.\n",
    "\n",
    "- What other stop words do you think there are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk; nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Use the list `stop` of English stopwords to remove stopwords from our dataset of Tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming/lemmatization\n",
    "\n",
    "Stemming and lemmatization both refer to remove morphological affixes on words. For example, if we stem the word \"grows\", we get \"grow\". If we stem the word \"running\", we get \"run\". We do this because often we care more about the core content of the word (i.e. that it has something to do with growth or running, rather than the fact that it's a third person present tense verb, or progressive participle).\n",
    "\n",
    "NLTK provides many algorithms for stemming. For English, a great baseline is the [Porter](https://github.com/nltk/nltk/blob/develop/nltk/stem/porter.py) algorithm, which is in spirit isn't that far from a bunch of regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grow'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('grows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'leav'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('leaves')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "import nltk; nltk.download('wordnet') # Download resource for working with WordNet via NLTK\n",
    "snowballer_stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "leav\n"
     ]
    }
   ],
   "source": [
    "print(snowballer_stemmer.stem('running'))\n",
    "print(snowballer_stemmer.stem('leaves'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaf\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('leaves'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Use the Porter stemmer to stem each word in the tweet dataset after having removed stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging\n",
    "\n",
    "POS tagging means assigning each token a part-of-speech (e.g. noun, verb, adjective, etc.). Again, there are many different [alternatives](https://github.com/nltk/nltk/tree/develop/nltk/tag), but NLTK keeps its recommended POS tagger available through the function `pos_tag`. The tagger expects a list of tokens as input.When doing POS tagging, it is advisable **not** to remove stop words beforehand (although you are free to do it afterwards)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I bought these for my brother in another state and accidentally had them shipped to me.  I am so glad that I did because then I had the opportunity to try them.  By the time I reordered some for him, they were out of stock.  They are in stock now, but will arrive after Christmas.  Great taste, quality. I have not found one bad one yet!'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "single_review = reviews[3]\n",
    "single_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('bought', 'VBD'),\n",
       " ('these', 'DT'),\n",
       " ('for', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('brother', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('another', 'DT'),\n",
       " ('state', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('accidentally', 'RB'),\n",
       " ('had', 'VBD'),\n",
       " ('them', 'PRP'),\n",
       " ('shipped', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('me', 'PRP'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('so', 'RB'),\n",
       " ('glad', 'JJ'),\n",
       " ('that', 'IN'),\n",
       " ('I', 'PRP'),\n",
       " ('did', 'VBD'),\n",
       " ('because', 'IN'),\n",
       " ('then', 'RB'),\n",
       " ('I', 'PRP'),\n",
       " ('had', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('opportunity', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('try', 'VB'),\n",
       " ('them', 'PRP'),\n",
       " ('.', '.'),\n",
       " ('By', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('time', 'NN'),\n",
       " ('I', 'PRP'),\n",
       " ('reordered', 'VBD'),\n",
       " ('some', 'DT'),\n",
       " ('for', 'IN'),\n",
       " ('him', 'PRP'),\n",
       " (',', ','),\n",
       " ('they', 'PRP'),\n",
       " ('were', 'VBD'),\n",
       " ('out', 'IN'),\n",
       " ('of', 'IN'),\n",
       " ('stock', 'NN'),\n",
       " ('.', '.'),\n",
       " ('They', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('in', 'IN'),\n",
       " ('stock', 'NN'),\n",
       " ('now', 'RB'),\n",
       " (',', ','),\n",
       " ('but', 'CC'),\n",
       " ('will', 'MD'),\n",
       " ('arrive', 'VB'),\n",
       " ('after', 'IN'),\n",
       " ('Christmas', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Great', 'NNP'),\n",
       " ('taste', 'NN'),\n",
       " (',', ','),\n",
       " ('quality', 'NN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('not', 'RB'),\n",
       " ('found', 'VBN'),\n",
       " ('one', 'CD'),\n",
       " ('bad', 'JJ'),\n",
       " ('one', 'CD'),\n",
       " ('yet', 'RB'),\n",
       " ('!', '.')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(single_review)\n",
    "import nltk; nltk.download('averaged_perceptron_tagger')\n",
    "tagged_review = pos_tag(tokens)\n",
    "tagged_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Below I've read in the text of Austen's _Pride and Prejudice_ into a variable called `pride`. Preprocess using the following steps:\n",
    "\n",
    "- Strip whitespace\n",
    "- Replace all numbers with '0'\n",
    "- Tokenize\n",
    "- Tag each token with a POS tag\n",
    "\n",
    "Make sure you know:\n",
    "- What type is the result?\n",
    "- What type is each element of the result?\n",
    "- What type are the elements of the elements of the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pride = read_pride()[679:684814]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTM/TF-IDF\n",
    "\n",
    "Document term matrix and Term Frequency-Inverse Document Frequency are common preprocessing steps for taking tokenized texts and turning them into numerical features, ready for supervised machine learning models. Scikit-learn is the standard method of using DTM and TF-IDF in Python. They have two main classes for this: [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today we express our deepest gratitude to all those who have served in our armed forces. HASHTAG URL ',\n",
       " 'Busy day planned in New York. Will soon be making some very important decisions on the people who will be running our government!',\n",
       " 'Love the fact that the small groups of protesters last night have passion for our great country. We will all come together and be proud!',\n",
       " 'Just had a very open and successful presidential election. Now professional protesters, incited by the media, are protesting. Very unfair!']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whitespace_pattern = r'\\s+'\n",
    "clean = [re.sub(url_pattern, URL_SIGN, t) for t in tweets]\n",
    "clean = [re.sub(hashtag_pattern, HASHTAG_SIGN, t) for t in clean]\n",
    "clean = [re.sub(digit_pattern, DIGIT_SIGN, t) for t in clean]\n",
    "clean = [re.sub(whitespace_pattern, ' ', t) for t in clean]\n",
    "clean[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7375x10046 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 113679 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "count = CountVectorizer()\n",
    "X = count.fit_transform(clean)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.toarray()[:5,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7375x10046 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 113679 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(clean)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.toarray()[:5,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on DTM/TF-IDF\n",
    "We will use Python's scikit-learn package learn to make a document term matrix from a .csv Music Reviews dataset (collected from MetaCritic.com). We will then use the DTM and a word weighting technique called tf-idf (term frequency inverse document frequency) to identify important and discriminating words within this dataset (utilizing the Pandas package). The illustrating question: **what words distinguish reviews of Rap albums, Indie Rock albums, and Jazz albums?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['While For Baltimore proves they can still write a grade A banger when they put their mind to it, too many songs are destined to have \"must try harder\" stamped on their report card. [13 Oct 2012, p.52]',\n",
       " 'There\\'s nothing fake about the purgatorial narrative of songs such as \"Nobody Knows My Trouble\" and \"My Diamond Is Too Rough.\" [Feb 2015, p.74]',\n",
       " \"All life's disastrous lows are here on a career-high album. [Nov 2014, p.121]\",\n",
       " 'With Doris, Odd Future’s Odysseus is finally back and chasing the ghosts out of his head.',\n",
       " 'Though Giraffe is definitely Echoboy\\'s most immediate and cohesive work, it\\'s not perfect: the album takes a misguided turn toward the dark and overwrought on songs like \"Lately Lonely\" and \"Wasted Spaces,\" both of which recall the harsher moments of Primal Scream\\'s Evil Heat.']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music = read_music()\n",
    "music[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge\n",
    "\n",
    "Remove all the digits from `music`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_digit(comment):\n",
    "    return ''.join([ch for ch in comment if not ch.isdigit()])\n",
    "\n",
    "no_digits = [remove_digit(comment) for comment in music]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer Function\n",
    "\n",
    "Our next step is to turn the text into a document term matrix using the scikit-learn function called `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countvec = CountVectorizer()\n",
    "sparse_dtm = countvec.fit_transform(no_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We made a DTM! Let's look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5001x16139 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 124340 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This format is called Compressed Sparse Format. It save a lot of memory to store the dtm in this format, but it is difficult to look at for a human. To illustrate the techniques in this lesson we will first convert this matrix back to a Pandas DataFrame, a format we're more familiar with. For larger datasets, you will have to use the Compressed Sparse Format. Putting it into a DataFrame, however, will enable us to get more comfortable with Pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dtm = pd.DataFrame(sparse_dtm.toarray(), columns=countvec.get_feature_names())\n",
    "#dtm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "I've read in a bunch of Jane Austen books into a variable called `books`, which is a list of strings and each string is an entire book. Turn them into a DTM. What will be the rows and columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUSTEN_DIR = os.path.join(DATA_DIR, 'austen', '*.txt')\n",
    "fnames = glob.glob(AUSTEN_DIR)\n",
    "books = []\n",
    "for fname in fnames:\n",
    "    with open(fname) as f:\n",
    "        text = f.read()\n",
    "    books.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF scores\n",
    "\n",
    "How to find distinctive words in a corpus is a long-standing question in text analysis? Today, we'll learn one simple approach to this: TF-IDF. The idea behind words scores is to weight words not just by their frequency, but by their frequency in one document compared to their distribution across all documents. Words that are frequent, but are also used in every single document, will not be distinguising. We want to identify words that are unevenly distributed across the corpus.\n",
    "\n",
    "One of the most popular ways to weight words (beyond frequency counts) is `tf-idf score`. By offsetting the frequency of a word by its document frequency (the number of documents in which it appears) will in theory filter out common terms such as 'the', 'of', and 'and'.\n",
    "\n",
    "Traditionally, the inverse document frequency is calculated as such:\n",
    "\n",
    "number_of_documents / number_documents_with_term\n",
    "\n",
    "so:\n",
    "\n",
    "tfidf_word1 = word1_frequency_document1 * (number_of_documents / number_document_with_word1)\n",
    "\n",
    "You can, and often should, normalize the numerator: \n",
    "\n",
    "tfidf_word1 = (word1_frequency_document1 / word_count_document1) * (number_of_documents / number_document_with_word1)\n",
    "\n",
    "We can calculate this manually, but scikit-learn has a built-in function to do so. This function also uses log frequencies, so the numbers will not correspond excactly to the calculations above. We'll use the [scikit-learn calculation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html), but a challenge for you: use Pandas to calculate this manually.\n",
    "\n",
    "### TF-IDFVectorizer Function\n",
    "\n",
    "To do so, we simply do the same thing we did above with CountVectorizer, but instead we use the function TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5001x16139 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 124340 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfvec = TfidfVectorizer()\n",
    "sparse_tfidf = tfidfvec.fit_transform(no_digits)\n",
    "sparse_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf = pd.DataFrame(sparse_tfidf.toarray(), columns=tfidfvec.get_feature_names())\n",
    "# tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Distinctive Words\n",
    "\n",
    "What can we do with this? These scores are best used when you want to identify distinctive words for individual documents, or groups of documents, compared to other groups or the corpus as a whole. To illustrate this, let's compare three genres and identify the most distinctive words by genre.\n",
    "\n",
    "First we add in a column of genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'TfidfVectorizer' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-17f30d02b46d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtfidf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'genre_'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'genre'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'TfidfVectorizer' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "fname = os.path.join(DATA_DIR, 'music_reviews.csv')\n",
    "reviews = pd.read_csv(fname, sep='\\t')\n",
    "\n",
    "tfidf['genre_'] = reviews['genre']\n",
    "tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rap = tfidf[tfidf['genre_']=='Rap']\n",
    "indie = tfidf[tfidf['genre_']=='Indie']\n",
    "jazz = tfidf[tfidf['genre_']=='Jazz']\n",
    "\n",
    "rap.max(numeric_only=True).sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indie.max(numeric_only=True).sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jazz.max(numeric_only=True).sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go! A method of identifying distinctive words. You notice there are some proper nouns in there. How might we remove those if we're not interested in them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things we didn't cover\n",
    "\n",
    "- Named entity recognition\n",
    "- Syntactic parsing\n",
    "- Information extraction\n",
    "- Removing markup from HTML\n",
    "- Extracting numerical features\n",
    "- SpaCy"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
